import logging
import os
from datetime import timedelta, datetime
from time import time
from tempfile import TemporaryDirectory

import pandas as pd
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor
from clickhouse_driver import Client
from airflow import DAG
from airflow_clickhouse_plugin.operators.clickhouse_operator import ClickHouseOperator
from airflow_clickhouse_plugin.hooks.clickhouse_hook import ClickHouseHook
from sqlalchemy import create_engine


# def get_sqlalchemy_engine():
#     ch_hook = ClickHouseHook()
#     uri = ch_hook.get_connection(conn_id=clickhouse_connection['clickhouse_conn_id']).get_uri()
#     # engine = create_engine(
#     #     'clickhouse+http://localhost/test',
#     #     connect_args={'http_session': Session()}
#     # )
#     return create_engine(uri)


clickhouse_connection = {
    "clickhouse_conn_id": "clickhouse_conn"
}

default_args = {
    "owner": "airflow",
    "retires": 5,
    "retry_delay": timedelta(minutes=5),
    "tags": ["project", "clickhouse"]
}
s3_file_key = "free_company_dataset.csv"
database_name = "catalogue"
table_name = "Companies"
bucket_name = "catalogue-companies"  # TODO auto create this bucket
s3_hook = S3Hook(aws_conn_id="s3_minio_conn")
ch_hook = ClickHouseHook(**clickhouse_connection)


def insert_chunk(table_name, df, client):
    client.insert_dataframe(
        f"""INSERT INTO {database_name}."{table_name}" VALUES""",
        df,
        settings=dict(use_numpy=True),
    )


def csv_to_sql():
    # download file into temporary directory
    with TemporaryDirectory() as temp_dir_name:
        logging.info(f"{s3_file_key} Created temp dir")
        downloaded_filename = s3_hook.download_file(key=s3_file_key,
                                                    bucket_name=bucket_name,
                                                    local_path=temp_dir_name,
                                                    preserve_file_name=True,
                                                    use_autogenerated_subdir=False,
                                                    )
        logging.info(f"{s3_file_key} Written temp file {downloaded_filename}")
        file_path = os.path.join(temp_dir_name, downloaded_filename)
        CHUNK_SIZE = 100_000
        df_iter = pd.read_csv(file_path, iterator=True, chunksize=CHUNK_SIZE, escapechar="\\")  # chunksize is number of rows
        df = next(df_iter)
        client: Client = ch_hook.get_conn()
        client.execute("SET max_partitions_per_insert_block = 300;")
        # https://stackoverflow.com/questions/31071952/generate-sql-statements-from-a-pandas-dataframe
        stmt_create_table = f"""
        CREATE TABLE IF NOT EXISTS {database_name}."{table_name}" (
        "country" TEXT,
          "founded" INT,
          "id" TEXT,
          "industry" TEXT,
          "linkedin_url" TEXT,
          "locality" TEXT,
          "name" TEXT,
          "region" TEXT,
          "size" TEXT,
          "website" TEXT
        ) 
        ENGINE = MergeTree 
        PRIMARY KEY "id"
        PARTITION BY "country";
        """
        logging.info(f"stmt_create_table: {stmt_create_table}")
        client.execute(stmt_create_table)
        logging.info(f"{s3_file_key} Created database table {table_name}")
        start_t = time()
        # https://stackoverflow.com/questions/58422110/pandas-how-to-insert-dataframe-into-clickhouse
        insert_chunk(table_name, df, client)
        end_t = time()
        logging.info(f"The first {CHUNK_SIZE} rows took {end_t - start_t}s to insert")
        chunk_count = 1
        while True:
            try:
                df = next(df_iter)
                insert_chunk(table_name, df, client)
                chunk_count += 1
            except StopIteration:
                logging.info(f"{s3_file_key} completed insertion into {table_name}")
                break
            except pd.errors.ParserError as e:
                logging.error(f"Chunk number {chunk_count}: between row {CHUNK_SIZE*chunk_count} and row {CHUNK_SIZE * (chunk_count +1)}")
                raise e


with DAG(dag_id="companies_to_clickhouse_v18", default_args=default_args,
         start_date=datetime.now(), schedule_interval="@once", catchup=False) as dag:
    sense_s3_file_add = S3KeySensor(task_id="sensor_minio_s3",
                                    aws_conn_id="s3_minio_conn",
                                    bucket_name=bucket_name,
                                    bucket_key="free_company_dataset.csv",
                                    poke_interval=5,  # secs
                                    timeout=10*60)

    create_ch_database = ClickHouseOperator(task_id="create_companies_db",
                                            **clickhouse_connection,
                                            sql=f"""
                              CREATE DATABASE IF NOT EXISTS {database_name};
                              """)

    mv_csv_to_ch_table = PythonOperator(task_id="csv_to_sql", python_callable=csv_to_sql)

    sense_s3_file_add >> create_ch_database >> mv_csv_to_ch_table
