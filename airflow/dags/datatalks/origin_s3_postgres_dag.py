import logging
import os
import re
import time
from datetime import datetime
from tempfile import NamedTemporaryFile, TemporaryDirectory
import pandas as pd
import pyarrow.parquet as pq
import requests
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datatalks.scripts.TlcTripScraper import TlcTripScraper
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

default_args = {
    "tags": ["talks"]
}

s3_hook = S3Hook(aws_conn_id="s3_minio_conn")
bucket_name = "yellow-taxi-trips"  # TODO auto create this bucket
MAX_N_TABLES = 10 # number in range of number of links or empty string


def save_trip_files_to_s3(ti):
    """
    gets links from task scrape_links_tlc and saves all the files pointing to the links
    :param ti: The task instance
    """
    links: list = ti.xcom_pull(task_ids='scrape_links_tlc')
    logging.info(f"Scrapped {len(links)} links")
    print("links[0]", links[0])
    keys = s3_hook.list_keys(bucket_name=bucket_name)
    for link in links[:MAX_N_TABLES]:
        filename = link.split("/")[-1]
        if filename in keys:
            logging.info(f"{filename} Already in bucket {bucket_name} — skipping")
        else:
            with requests.get(link) as r:
                r.raise_for_status()
                if "Content-Disposition" in r.headers.keys():
                    filename = re.findall("filename=(.+)", r.headers["Content-Disposition"])[0]
                with NamedTemporaryFile(mode="wb", delete=False) as temp_file:
                    logging.info(f"{filename} Opened empty file")
                    with temp_file as f:  # closes the file
                        for chunk in r.iter_content(chunk_size=512):
                            f.write(chunk)
                        f.flush()
                        logging.info(f"{filename} Written temporary file")
                s3_hook.load_file(filename=temp_file.name, key=filename,
                                  bucket_name=bucket_name,
                                  replace=True)
                logging.info(f"{filename} Uploaded to {bucket_name}")
                try:  # deleting temporary file
                    os.remove(temp_file.name)
                    logging.info(f"{filename} Removed temporary file")
                except OSError:
                    pass
    logging.info("Saved trip files to s3 — Done")


def get_postgres_engine():
    hook = PostgresHook(postgres_conn_id="postgres_test_db_conn")
    return hook.get_sqlalchemy_engine()


def s3_to_postgres():
    """
    downloads all objects from bucket
    and creates corresponding tables in postgres provided by get_postgres_engine
    """
    keys = s3_hook.list_keys(bucket_name=bucket_name)
    engine = get_postgres_engine()
    with engine.connect() as connection:
        for key in keys[:MAX_N_TABLES]:
            table_name = key.replace(".parquet", "")
            if engine.dialect.has_table(connection, table_name):
                logging.info(f"{key} Table already exists — skipping")
            else:
                with TemporaryDirectory() as temp_dir_name:
                    logging.info(f"{key} Created temp dir")
                    downloaded_filename = s3_hook.download_file(key=key, bucket_name=bucket_name,
                                                                local_path=temp_dir_name, preserve_file_name=True,
                                                                use_autogenerated_subdir=False)
                    logging.info(f"{key} Written temp file {downloaded_filename}")
                    parquet_file = pq.ParquetFile(os.path.join(temp_dir_name, downloaded_filename))
                    # df = pd.read_parquet(temp_file.name)
                    for batch in parquet_file.iter_batches():
                        batch_df = batch.to_pandas()
                        batch_df.to_sql(name=table_name, con=engine, if_exists="append")
                logging.info(f"{key} Done to_sql")


with DAG(dag_id="yellow_taxi_data_ingestion_v15", default_args=default_args,
         start_date=datetime.now(), schedule_interval="@once") as dag:
    task_scrape_links = PythonOperator(task_id="scrape_links_tlc", python_callable=TlcTripScraper.run)
    task_save_trip_files_to_s3 = PythonOperator(task_id="save_trip_files_to_s3", python_callable=save_trip_files_to_s3)
    task_s3_to_postgres = PythonOperator(task_id="s3_to_postgres",
                                         python_callable=s3_to_postgres)

    task_scrape_links >> task_save_trip_files_to_s3 >> task_s3_to_postgres
